<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Sharath</title>

  <meta name="author" content="Sharath">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon" type="image/png" href="images/icon.png">
</head>

<body>
  <table style="width:100%;max-width:900px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Sharath Chandra Raparthy</name>
              </p>
              <p>I am an AI Resident at Meta AI where I work with Roberta Raileanu and Mikael Henaff. Prior to this, I was a Masters (with thesis) student at <a href="https://mila.quebec/en/">Mila</a> where I was supervised by  <a href="https://sites.google.com/site/irinarish/">Prof. Irina Rish.</a>
              I also spent some time at <a href="https://www.recursion.com/">Recursion</a> where I worked on GFlowNets for generative chemistry. I am interested in the general area of reinforcement learning and I am specifically interested in building efficient algorithms which are geared towards sample efficiency in the continual RL scenarios.
              </p>

              <p>
                Besides research, I enjoy photography, reading books and cooking.

              </p>
<!--              <button onclick="research()">Research Interest</button>-->

<!--              <p>-->
<!--                As a cool and fun side project, I started jotting down my understanding of the research papers I read and also some cool concepts in math/ML I find interesting. I am planning to publish at least one research paper notes on a weekly basis.  <a href="https://github.com/SharathRaparthy/research-readings">You can checkout my project here.</a>-->
<!--              </p>-->
              <p style="text-align:center">
                <a target="_blank" href="sharathraparthy@gmail.com"> Email</a> &nbsp;/&nbsp;
                <a href="https://github.com/SharathRaparthy">GitHub</a> &nbsp;/&nbsp;
                <a href="https://scholar.google.ca/citations?user=S1R0_UMAAAAJ&hl=en">Google Scholar</a>
              </p>
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <a href="images/sharath_new.jpg"><img style="width:100%;max-width:100%" alt="profile photo" src="images/sharath_new.jpg" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>
        <!-- <h2>Background</h2> -->
        <p>
        <!-- with <a href="https://webdocs.cs.ualberta.ca/~dale/">Dale Schuurmans</a> and <a href="https://www.afaust.info/">Aleksandra Faust</a> in similar areas. -->
        <!-- I was fortunate to also work on consumer privacy rights and legislation with folks from <a href="https://www.law.georgetown.edu/privacy-technology-center">Georgetown's Center on Privacy and Technology.</a></p> -->
              </p>
              <p>
              </p>
        <h2>News</h2>

            <ul>
                    <li> <b> Aug 2022:</b> Joining <a href="https://ai.facebook.com/">MetaAI</a> as an AI Resident</li>
                    <li> <b> Apr 2022:</b> Joining <a href="https://www.recursion.com/">Recursion</a> as a research intern</li>
                    <li> <b> Oct 2021:</b> Co-organizing <a href="https://paperswithcode.com/rc2021">Machine Learning Reproducibility Challenge</a></li>
                    <li> <b> Oct 2021:</b> Our work on <a href="https://arxiv.org/abs/2110.09419">compositional attention</a> got accepted at ICLR 2022 as a <b style='color:red !important;'>spotlight presentation</b>.   </li>
                    <li> <b> Oct 2021:</b> New preprint out: <a href="https://arxiv.org/abs/2110.09419">Continual Learning In Environments With Polynomial Mixing Times</a>   </li>
<!--                    <li> <b> Nov 2021:</b> Received Thesis Excellence (DIRO: Excellence-Rédaction) Scholarship to pursue my research.</li>-->
<!--                    <li> <b> Feb 2021:</b> Started a <a href="https://github.com/SharathRaparthy/research-readings">fun side project</a> </li>-->
                    <li> <b> Sep 2020:</b> Started my masters at <a href="https://mila.quebec/">Mila</a> </li>
<!--                    <li> <b> July 2020:</b> Our work CuNAS has been accepted to <a href="https://sim2real.github.io/">RSS, Sim2Real workshop</a></li>-->
<!--                    <li> <b> Feb 2020:</b> Our two works, <a href="https://arxiv.org/abs/2002.07911">SS-ADR</a> and <a href="https://arxiv.org/abs/2002.07956"> Meta-ADR</a> have been accepted to <a href="http://www.betr-rl.ml/2020/">ICLR BeTR-RL workshop</a></li>-->
<!--                    <li> <b> Feb 2020:</b> Excited to announce two preprints: <a href="https://arxiv.org/abs/2002.07911">SS-ADR</a> and <a href="https://arxiv.org/abs/2002.07956"> Meta-ADR</a></li>-->
<!--                  <li> <b> Jan 2020:</b> Started working with <a href="https://sites.google.com/site/irinarish/">Prof. Irina Rish</a> on Meta-Reinforcement Learning </li>-->
<!--                   <li> <b> July 2019:</b> Attended and volunteered the multi-disciplinary conference on <a href="http://rldm.org/">Reinforcement Learning and Decision Making, RLDM-2019</a> </li>-->
<!--                   <li><b> July 2019:</b> Started my internship at <a href="https://mila.quebec/">Montreal Institute for Learning Algorithms (Mila)</a> under <a href="http://liampaull.ca/">Prof. Liam Paull</a> </li>-->
<!--                   <li><b> December 2018:</b> Our paper titled “Explicit Sequence Proximity Models for Hidden State Identification” is accepted to <a href="https://sites.google.com/site/rlponips2018/home/call-for-papers?authuser=0">-->
<!--                  NeurIPS 2018 workshop on Reinforcement Learning under Partial Observability </a>  </li>-->
<!--                   <li><b> October 2018:</b> I am happy to announce that I have been accepted for PyTorch Scholarship Challenge from Facebook. </li>-->
                  </ul>
            <!--<h2>Research 	&#129302;</h2>-->
        <h2>Research</h2>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

                <!--
          <tr onmouseout="nightsight_stop()" onmouseover="nightsight_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/todo.png" alt="hpp" style="border-style: none" width="200">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="todo_link">
                <papertitle>TODO_paper_title</papertitle>
              </a>
              <br>
              <strong>Rose E. Wang</strong>,
              TODO,
              <a href="https://cocolab.stanford.edu/ndg">Noah Goodman</a>
              <br>
              <em>TODO conference venue</em>.
              <br>
              <p>TODO tldr</p>
            </td>
          </tr>
                -->
          <tr onmouseout="comp_attention_stop()" onmouseover="comp_attention_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/comp-atten.png" alt="kts" style="border-style: none" width="200">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/2110.09419v1">
                <papertitle>Compositional Attention: Disentangling Search and Retrieval</papertitle>
              </a>
              <br>
              Sarthak Mittal
              <strong>Sharath Chandra Raparthy</strong>, Irina Rish, Yoshua Bengio and Guillaume Lajoie
              <br>
              <em>International Conference for Learning Representations (ICLR) 2022</em>.
              <br>
              <b style="color:red">Spotlight Presentation</b>
              <br>
              [
              <a href="https://arxiv.org/abs/2110.09419v1">Paper</a>
               /
              <a href="https://github.com/sarthmit/Compositional-Attention">Code</a>
               ]
              <br>
              <p>
              We view the standard Multi-Head attention mechanism from the "Search-Retrieval"  perspective and highlight the rigid associations of keys and values. We propose a new drop-in replacement mechanism, Compositional Attention, where the redundancies highlighted are addressed by disentangling the Searches and Retrievals and composing them dynamically in a context dependent way.
</p>
            </td>
          </tr>

          <tr onmouseout="nightsight_stop()" onmouseover="nightsight_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/figure-1-draft-11.png" alt="hpp" style="border-style: none" width="200">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/2112.07066">
                  <papertitle>Continual Learning In Environments With Polynomial Mixing Times</papertitle>
              </a>
              <br>
              Matthew Riemer*, <strong>Sharath Chandra Raparthy*</strong>, Ignacio Cases, Gopeshh Subbaraj, Maximilian Puelma Touzel and Irina Rish
              <br>
              <em>In submission</em>
                <br>
              [
              <a href="https://arxiv.org/abs/2112.07066">Paper</a> /
              <a href="https://github.com/sharathraparthy/polynomial-mixing-times">Code</a>]
              <br>
              <p>In this work, we concentrate on the major contributor to poor scaling, "Mixing time" of a markov chain induced by a policy.  Mixing times, when ignored, can create myopic biases in the optimization and hence is an impediment to the success in the continual RL problems of greatest interest. We categorize the continual RL problems as Scalable MDPs and formally demonstrate that these exhibit polynomial mixing times. We comment on how exisiting RL algorithms face difficulties in this regime and propose three algorithms which clearly demonstrate sample efficiency.   </p>
            </td>
          </tr>
         <tr onmouseout="nightsight_stop()" onmouseover="nightsight_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/meta-adr.png" alt="hpp" style="border-style: none" width="200">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/2112.07066">
                  <papertitle>Curriculum in Gradient-Based Meta-Reinforcement Learning</papertitle>
              </a>
              <br>
                Bhairav Mehta, Tristan Deleu*, <strong>Sharath Chandra*</strong> Christopher Pal, Liam Paull
                <br>
              <em>ICLR BeTR-RL workshop (2021)</em>
                <br>
              [
              <a href="https://arxiv.org/abs/2002.07956">Paper</a>]
                <br>
                <p>
                    In this work we study the under-studied parameter in meta learning, "Task Distributions". We show that Model Agnostic Meta-Learning (MAML) is sensitive to task distributions, and learning a curriculum of tasks instead of uniformly sampling helps the adaptation performance substantially.

                </p>
            </td>
          </tr>
<!--        <tr onmouseout="nightsight_stop()" onmouseover="nightsight_start()">-->
<!--            <td style="padding:20px;width:25%;vertical-align:middle">-->
<!--              <img src="images/meta-adr.png" alt="hpp" style="border-style: none" width="200">-->
<!--            </td>-->
<!--            <td style="padding:20px;width:75%;vertical-align:middle">-->
<!--              <a href="https://arxiv.org/abs/2112.07066">-->
<!--                  <papertitle>Curriculum in Gradient-Based Meta-Reinforcement Learning</papertitle>-->
<!--              </a>-->
<!--              <br>-->
<!--                Bhairav Mehta, Tristan Deleu*, <strong>Sharath Chandra*</strong> Christopher Pal, Liam Paull-->
<!--                <br>-->
<!--              <em>ICLR BeTR-RL workshop (2021)</em>-->
<!--                <br>-->
<!--              [-->
<!--              <a href="https://arxiv.org/abs/2002.07956">Paper</a>]-->
<!--                <br>-->
<!--                <p>-->
<!--                    In this work we study the under-studied parameter in meta learning, "Task Distributions". We show that Model Agnostic Meta-Learning (MAML) is sensitive to task distributions, and learning a curriculum of tasks instead of uniformly sampling helps the adaptation performance substantially.-->

<!--                </p>-->
<!--            </td>-->
<!--          </tr>-->
        <tr onmouseout="nightsight_stop()" onmouseover="nightsight_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/r:ss.png" alt="hpp" style="border-style: none" width="200">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/2112.07066">
                  <papertitle>CuNAS - CUriosity-driven Neural-Augmented Simulator</papertitle>
              </a>
              <br>
                <strong>Sharath Chandra Raparthy</strong>, Melissa Mozifian, Liam Paull and Florian Golemo
                <br>
              <em>RSS Sim2Real workshop (2021)</em>
                <br>
              [
              <a href="https://docs.google.com/presentation/d/1nVbt0iQKFTOgHEQLLHbn1Wy3bMs_mWpyzfc0aZsN30U/edit?usp=sharing">Slides</a> /
              <a href="https://www.youtube.com/watch?v=Tlf5RG3OPF8&list=PL4BpvvbNDc3SxmswMbOljlUcCQJQ6eFDL&index=6">Talk</a>]
              <br>
              <p>Transfer of policies from simulation to physical robots is an important open problem in deep reinforcement learning. Prior work has introduced the model-based Neural-Augmented Simulator (NAS) method, which uses task-independent data to create a model of the differences between simulated and real robot. In this work, we show that this method is sensitive to the sampling of motor actions and the control frequency. To overcome this problem, we propose a simple extension based on artificial curiosity. We demonstrate on a physical robot, that this leads to a better exploration of the state space and consequently better transfer performance when compared to the NAS baseline.</p>
            </td>
          </tr>

        </tbody></table>

      </td>
    </tr>
  </table>
  Template from <a href="https://jonbarron.info/">this website</a>.
</body>

</html>