<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Sharath</title>

  <meta name="author" content="Sharath">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon" type="image/png" href="images/icon.png">
</head>

<body>
  <table style="width:100%;max-width:900px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Sharath Chandra Raparthy</name>
              </p>
              <p>
                I am currently an AI Resident at <a href="https://ai.meta.com/">FAIR at Meta</a>, collaborating closely with Roberta Raileanu and Mikael Henaff. My research focus has evolved to primarily include Large Language Model (LLM) reasoning, tool fine-tuning, and in-context learning. Additionally, I am engaged in exploring the dimensions of LLM safety to ensure responsible and ethical AI development.
            </p>
            <p>
                Before joining Meta, I completed a Master's (with thesis) at <a href="https://mila.quebec/en/">Mila</a> under the guidance of Prof. <a href="https://sites.google.com/site/irinarish/">Irina Rish</a>. My academic journey also included a valuable stint at <a href="https://www.recursion.com">Recursion</a>, where my projects were centered around GFlowNets in the realm of generative chemistry. Although my earlier research was anchored in reinforcement learning, particularly in developing efficient algorithms for sample efficiency in continual RL scenarios, my interests have since shifted to the cutting-edge domains of LLMs.
            </p>
            <p>
                Outside the realm of AI research, my passions include photography, long-distance running, reading and cooking. 
            </p>
            
<!--              <button onclick="research()">Research Interest</button>-->

<!--              <p>-->
<!--                As a cool and fun side project, I started jotting down my understanding of the research papers I read and also some cool concepts in math/ML I find interesting. I am planning to publish at least one research paper notes on a weekly basis.  <a href="https://github.com/SharathRaparthy/research-readings">You can checkout my project here.</a>-->
<!--              </p>-->
              <p style="text-align:center">
                <a target="_blank" href="sharathraparthy@gmail.com"> Email</a> &nbsp;/&nbsp;
                <a href="https://github.com/SharathRaparthy">GitHub</a> &nbsp;/&nbsp;
                <a href="https://scholar.google.ca/citations?user=S1R0_UMAAAAJ&hl=en">Google Scholar</a>
              </p>
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <a href="images/sharath_new.jpg"><img style="width:100%;max-width:100%" alt="profile photo" src="images/sharath_new.jpg" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>
        <!-- <h2>Background</h2> -->
        <p>
        <!-- with <a href="https://webdocs.cs.ualberta.ca/~dale/">Dale Schuurmans</a> and <a href="https://www.afaust.info/">Aleksandra Faust</a> in similar areas. -->
        <!-- I was fortunate to also work on consumer privacy rights and legislation with folks from <a href="https://www.law.georgetown.edu/privacy-technology-center">Georgetown's Center on Privacy and Technology.</a></p> -->
              </p>
              <p>
              </p>
        <h2>News</h2>

            <ul>
                    <li> <b> Apr 2024:</b> Super happy to release <a href="https://ai.meta.com/blog/meta-llama-3/"> Llama-3 preview models </a> is out.</li>
                    <li> <b> Mar 2024:</b> New preprint on <a href="https://arxiv.org/abs/2402.16822"> Rainbow Teaming: Open-Ended Generation of Diverse Adversarial Prompts </a> is out.</li>
                    <li> <b> Mar 2024:</b> New preprint on <a href="https://arxiv.org/abs/2403.04642"> GLoRe: When, Where, and How to Improve LLM Reasoning via Global and Local Refinements </a> is out.</li>
                    <li> <b> Mar 2024:</b> New preprint on <a href="https://arxiv.org/abs/2312.03801"> Teaching large language models to reason with reinforcement learning </a> is out.</li>
                    <li> <b> Feb 2024:</b> Featured on  <a href="https://www.talkrl.com/episodes/sharath-chandra-raparthy"> TalkRL podcast </a> to discuss our work on In-context Learning for Sequential Decision Making.</li>
                    <li> <b> Dec 2023:</b> New preprint on <a href="https://arxiv.org/abs/2312.03801"> Generalization to New Sequential Decision Making Tasks with In-Context Learning </a> is out.</li>
                    <li> <b> Oct 2022:</b> Our work<a href="https://arxiv.org/abs/2210.12765"> Multi-Objective GFlowNets</a> got accepted at ICML 2023</li>
                    <li> <b> Aug 2022:</b> Our work<a href="https://arxiv.org/abs/2110.09419"> Continual Learning In Environments With Polynomial Mixing Times</a> got accepted at NeurIPS 2022</li>
                    <li> <b> Aug 2022:</b> Co-organizing <a href="https://paperswithcode.com/rc2022">Machine Learning Reproducibility Challenge - 2022</a></li>
                    <li> <b> Aug 2022:</b> Joining <a href="https://ai.facebook.com/">MetaAI</a> as an AI Resident</li>
                    <li> <b> Apr 2022:</b> Joining <a href="https://www.recursion.com/">Recursion</a> as a research intern</li>
                    <li> <b> Oct 2021:</b> Co-organizing <a href="https://paperswithcode.com/rc2021">Machine Learning Reproducibility Challenge - 2021</a></li>
                    <li> <b> Oct 2021:</b> Our work on <a href="https://arxiv.org/abs/2110.09419">compositional attention</a> got accepted at ICLR 2022 as a <b style='color:red !important;'>spotlight presentation</b>.   </li>
                    <li> <b> Oct 2021:</b> New preprint out: <a href="https://arxiv.org/abs/2110.09419">Continual Learning In Environments With Polynomial Mixing Times</a>   </li>
<!--                    <li> <b> Nov 2021:</b> Received Thesis Excellence (DIRO: Excellence-R√©daction) Scholarship to pursue my research.</li>-->
<!--                    <li> <b> Feb 2021:</b> Started a <a href="https://github.com/SharathRaparthy/research-readings">fun side project</a> </li>-->
                    <li> <b> Sep 2020:</b> Started my masters at <a href="https://mila.quebec/">Mila</a> </li>
<!--                    <li> <b> July 2020:</b> Our work CuNAS has been accepted to <a href="https://sim2real.github.io/">RSS, Sim2Real workshop</a></li>-->
<!--                    <li> <b> Feb 2020:</b> Our two works, <a href="https://arxiv.org/abs/2002.07911">SS-ADR</a> and <a href="https://arxiv.org/abs/2002.07956"> Meta-ADR</a> have been accepted to <a href="http://www.betr-rl.ml/2020/">ICLR BeTR-RL workshop</a></li>-->
<!--                    <li> <b> Feb 2020:</b> Excited to announce two preprints: <a href="https://arxiv.org/abs/2002.07911">SS-ADR</a> and <a href="https://arxiv.org/abs/2002.07956"> Meta-ADR</a></li>-->
<!--                  <li> <b> Jan 2020:</b> Started working with <a href="https://sites.google.com/site/irinarish/">Prof. Irina Rish</a> on Meta-Reinforcement Learning </li>-->
<!--                   <li> <b> July 2019:</b> Attended and volunteered the multi-disciplinary conference on <a href="http://rldm.org/">Reinforcement Learning and Decision Making, RLDM-2019</a> </li>-->
<!--                   <li><b> July 2019:</b> Started my internship at <a href="https://mila.quebec/">Montreal Institute for Learning Algorithms (Mila)</a> under <a href="http://liampaull.ca/">Prof. Liam Paull</a> </li>-->
<!--                   <li><b> December 2018:</b> Our paper titled ‚ÄúExplicit Sequence Proximity Models for Hidden State Identification‚Äù is accepted to <a href="https://sites.google.com/site/rlponips2018/home/call-for-papers?authuser=0">-->
<!--                  NeurIPS 2018 workshop on Reinforcement Learning under Partial Observability </a>  </li>-->
<!--                   <li><b> October 2018:</b> I am happy to announce that I have been accepted for PyTorch Scholarship Challenge from Facebook. </li>-->
                  </ul>
            <!--<h2>Research 	&#129302;</h2>-->
        <h2>Research</h2>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

                <!--
          <tr onmouseout="nightsight_stop()" onmouseover="nightsight_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/todo.png" alt="hpp" style="border-style: none" width="200">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="todo_link">
                <papertitle>TODO_paper_title</papertitle>
              </a>
              <br>
              <strong>Rose E. Wang</strong>,
              TODO,
              <a href="https://cocolab.stanford.edu/ndg">Noah Goodman</a>
              <br>
              <em>TODO conference venue</em>.
              <br>
              <p>TODO tldr</p>
            </td>
          </tr>
                -->
              <tr onmouseout="comp_attention_stop()" onmouseover="comp_attention_start()">
                  <td style="padding:20px;width:25%;vertical-align:middle">
                    <img src="images/llama-3.png" alt="kts" style="border-style: none" width="200">
                  </td>
                  <td style="padding:20px;width:75%;vertical-align:middle">
                    <a href="https://ai.meta.com/blog/meta-llama-3/">
                      <papertitle>Llama-3 Preview Models</papertitle>
                    </a>
                    <br>
                    Llama Team
                    <br>
                    <em></em>
                    <br>
                    <!-- <b style="color:red">Spotlight Presentation</b> -->
                    <br>
                    [
                    <a href="https://ai.meta.com/blog/meta-llama-3/">Blog</a>
                     <!-- / -->
                    <!-- <a href="https://sites.google.com/view/rainbow-teaming">Website</a> -->
                     <!-- / -->
                     <!-- <a href="https://twitter.com/_samvelyan/status/1762519344943104195">tl;dr</a> -->
                     ]
                    <br>
                    <br>
                    <p>
                      We introduce Llama 3 family of large language models (LLMs), a collection of pretrained and instruction tuned generative text models in 8 and 70B sizes. We achieve SOTA performance for LLM models at these scales. 
                    </p>
                  </td>
              </tr>
              <tr onmouseout="comp_attention_stop()" onmouseover="comp_attention_start()">
                  <td style="padding:20px;width:25%;vertical-align:middle">
                    <img src="images/rainbow-teaming.png" alt="kts" style="border-style: none" width="200">
                  </td>
                  <td style="padding:20px;width:75%;vertical-align:middle">
                    <a href="https://arxiv.org/abs/2402.16822">
                      <papertitle>Rainbow Teaming: Open-Ended Generation of Diverse Adversarial Prompts</papertitle>
                    </a>
                    <br>
                    Mikayel Samvelyan*, <strong>Sharath Chandra Raparthy*</strong>, Andrei Lupu*, Eric Hambro, Aram H. Markosyan, Manish Bhatt, Yuning Mao, Minqi Jiang, Jack Parker-Holder, Jakob Foerster, Tim Rockt√§schel, Roberta Raileanu
                    <br>
                    <em>Arxiv</em>.
                    <br>
                    <!-- <b style="color:red">Spotlight Presentation</b> -->
                    <br>
                    [
                    <a href="https://arxiv.org/abs/2402.16822">Paper</a>
                     /
                    <a href="https://sites.google.com/view/rainbow-teaming">Website</a>
                     /
                     <a href="https://twitter.com/_samvelyan/status/1762519344943104195">tl;dr</a>
                     ]
                    <br>
                    <br>
                    <p>
                    Introducing Rainbow Teaming, a new method for generating diverse adversarial prompts for LLMs via LLMs. It's a versatile tool üõ†Ô∏è for diagnosing model vulnerabilities across domains and creating data to enhance robustness & safety. 
                    </p>
                  </td>
              </tr>
              <tr onmouseout="comp_attention_stop()" onmouseover="comp_attention_start()">
                  <td style="padding:20px;width:25%;vertical-align:middle">
                    <img src="images/galore.png" alt="kts" style="border-style: none" width="200">
                  </td>
                  <td style="padding:20px;width:75%;vertical-align:middle">
                    <a href="https://arxiv.org/abs/2312.03801">
                      <papertitle>GLoRe: When, Where, and How to Improve LLM Reasoning via Global and Local Refinements</papertitle>
                    </a>
                    <br>
                    Alex Havrilla, <strong>Sharath Chandra Raparthy</strong>, Christoforus Nalmpantis, Jane Dwivedi-Yu, Maksym Zhuravinskyi, Eric Hambro, Roberta Railneau
                    <br>
                    <em>Arxiv</em>.
                    <br>
                    <!-- <b style="color:red">Spotlight Presentation</b> -->
                    <br>
                    [
                    <a href="https://arxiv.org/abs/2402.10963">Paper</a>
                     /
                    <a href="https://twitter.com/Dahoas1/status/1760021603105288550">tl;dr</a>
                     ]
                    <br>
                    <p>How to bootstrap the reasoning refinement capabilities of LLMs using synthetic data? Introducing "GLoRe: When, Where, and How to Improve LLM Reasoning via Global and Local Refinements". Applied on GSM8K we can improve a strong RL finetuned LLama-2 13B by 12%</p>
                  </td>
              </tr>
              <tr onmouseout="comp_attention_stop()" onmouseover="comp_attention_start()">
                  <td style="padding:20px;width:25%;vertical-align:middle">
                    <img src="images/gsm8k.png" alt="kts" style="border-style: none" width="200">
                  </td>
                  <td style="padding:20px;width:75%;vertical-align:middle">
                    <a href="https://arxiv.org/abs/2312.03801">
                      <papertitle>Teaching Large Language Models to Reason with Reinforcement Learning</papertitle>
                    </a>
                    <br>
                    Alex Havrilla, Yuqing Du, <strong>Sharath Chandra Raparthy</strong>, Christoforos Nalmpantis, Jane Dwivedi-Yu, Maksym Zhuravinskyi, Eric Hambro, Sainbayar Sukhbaatar, Roberta Raileanu
                    <br>
                    <em>Arxiv</em>.
                    <br>
                    <!-- <b style="color:red">Spotlight Presentation</b> -->
                    <br>
                    [
                    <a href="https://arxiv.org/abs/2403.04642">Paper</a>
                     /
                    <a href="https://twitter.com/Dahoas1/status/1766120506028359853">tl;dr</a>
                     ]
                    <br>
                    <p>
                    In this work, we set out to understand how different algorithms fare at improving LLM reasoning from feedback. We compare expert iteration, PPO, and return-conditioned RL using Llama-2 as the base model.
                    </p>
                  </td>
              </tr>
              <tr onmouseout="comp_attention_stop()" onmouseover="comp_attention_start()">
                  <td style="padding:20px;width:25%;vertical-align:middle">
                    <img src="images/ICL-SDM.gif" alt="kts" style="border-style: none" width="200">
                  </td>
                  <td style="padding:20px;width:75%;vertical-align:middle">
                    <a href="https://arxiv.org/abs/2312.03801">
                      <papertitle>Generalization to New Sequential Decision Making Tasks with In-Context Learning</papertitle>
                    </a>
                    <br>
                    <strong>Sharath Chandra Raparthy</strong>, Eric Hambro, Robert Kirk, Mikael Henaff, Roberta Raileanu
                    <br>
                    <em>Arxiv</em>.
                    <br>
                    <!-- <b style="color:red">Spotlight Presentation</b> -->
                    <br>
                    [
                    <a href="https://arxiv.org/abs/2312.03801">Paper</a>
                     /
                    <a href="">Code</a>
                     ]
                    <br>
                    <p>
                      Training autonomous agents to learn new tasks from few demonstrations is challenging, especially for sequential decision making which is sensitive to errors. In this paper, we show that training transformers on diverse offline datasets of trajectories enables in-context learning of out-of-distribution sequential decision tasks from just a handful of demonstrations.
                    </p>
                  </td>
              </tr>
            <tr onmouseout="comp_attention_stop()" onmouseover="comp_attention_start()">
                  <td style="padding:20px;width:25%;vertical-align:middle">
                    <img src="images/mogfn.png" alt="kts" style="border-style: none" width="200">
                  </td>
                  <td style="padding:20px;width:75%;vertical-align:middle">
                    <a href="https://arxiv.org/abs/2210.12765">
                      <papertitle>Multi-Objective GFlowNets</papertitle>
                    </a>
                    <br>
                    Moksh Jain, <strong>Sharath Chandra Raparthy</strong>, Alex Hernandez-Garcia, Jarrid Rector-Brooks, Yoshua Bengio, Santiago Miret, Emmanuel Bengio
                    <br>
                    <em>International Conference on Machine Learning (ICML) 2023</em>.
                    <br>
                    <!-- <b style="color:red">Spotlight Presentation</b> -->
                    <br>
                    [
                    <a href="https://arxiv.org/abs/2210.12765">Paper</a>
                     /
                    <a href="https://github.com/sarthmit/Compositional-Attention">Code</a>
                     ]
                    <br>
                    <p>
                      We examine the standard approach to multi-objective optimization in machine learning applications like drug discovery and material design from a fresh perspective, noting the failure of existing methods to achieve a diverse set of Pareto-optimal candidates. Motivated by the successful use of GFlowNets in single-objective settings, we introduce a new approach, Multi-Objective GFlowNets (MOGFNs), which features a novel Conditional GFlowNet to handle a variety of single-objective sub-problems derived from decomposing the multi-objective problem. Our research, the first to empirically test Conditional GFlowNets, shows that MOGFNs outperform existing methods in Hypervolume, R2-distance, and candidate diversity, even demonstrating their effectiveness in active learning settings.      </p>
                  </td>
                </tr>
          <tr onmouseout="comp_attention_stop()" onmouseover="comp_attention_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/comp-atten.png" alt="kts" style="border-style: none" width="200">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/2110.09419v1">
                <papertitle>Compositional Attention: Disentangling Search and Retrieval</papertitle>
              </a>
              <br>
              Sarthak Mittal
              <strong>Sharath Chandra Raparthy</strong>, Irina Rish, Yoshua Bengio and Guillaume Lajoie
              <br>
              <em>International Conference for Learning Representations (ICLR) 2022</em>.
              <br>
              <b style="color:red">Spotlight Presentation</b>
              <br>
              [
              <a href="https://arxiv.org/abs/2110.09419v1">Paper</a>
               /
              <a href="https://github.com/sarthmit/Compositional-Attention">Code</a>
               ]
              <br>
              <p>
              We view the standard Multi-Head attention mechanism from the "Search-Retrieval"  perspective and highlight the rigid associations of keys and values. We propose a new drop-in replacement mechanism, Compositional Attention, where the redundancies highlighted are addressed by disentangling the Searches and Retrievals and composing them dynamically in a context dependent way.
</p>
            </td>
          </tr>

          <tr onmouseout="nightsight_stop()" onmouseover="nightsight_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/figure-1-draft-11.png" alt="hpp" style="border-style: none" width="200">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/2112.07066">
                  <papertitle>Continual Learning In Environments With Polynomial Mixing Times</papertitle>
              </a>
              <br>
              Matthew Riemer*, <strong>Sharath Chandra Raparthy*</strong>, Ignacio Cases, Gopeshh Subbaraj, Maximilian Puelma Touzel and Irina Rish
              <br>
              <em>NeurIPS 2022</em>
                <br>
              [
              <a href="https://arxiv.org/abs/2112.07066">Paper</a> /
              <a href="https://github.com/sharathraparthy/polynomial-mixing-times">Code</a>]
              <br>
              <p>In this work, we concentrate on the major contributor to poor scaling, "Mixing time" of a markov chain induced by a policy.  Mixing times, when ignored, can create myopic biases in the optimization and hence is an impediment to the success in the continual RL problems of greatest interest. We categorize the continual RL problems as Scalable MDPs and formally demonstrate that these exhibit polynomial mixing times. We comment on how exisiting RL algorithms face difficulties in this regime and propose three algorithms which clearly demonstrate sample efficiency.   </p>
            </td>
          </tr>
         <tr onmouseout="nightsight_stop()" onmouseover="nightsight_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/meta-adr.png" alt="hpp" style="border-style: none" width="200">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/2112.07066">
                  <papertitle>Curriculum in Gradient-Based Meta-Reinforcement Learning</papertitle>
              </a>
              <br>
                Bhairav Mehta, Tristan Deleu*, <strong>Sharath Chandra*</strong> Christopher Pal, Liam Paull
                <br>
              <em>ICLR BeTR-RL workshop (2021)</em>
                <br>
              [
              <a href="https://arxiv.org/abs/2002.07956">Paper</a>]
                <br>
                <p>
                    In this work we study the under-studied parameter in meta learning, "Task Distributions". We show that Model Agnostic Meta-Learning (MAML) is sensitive to task distributions, and learning a curriculum of tasks instead of uniformly sampling helps the adaptation performance substantially.

                </p>
            </td>
          </tr>
<!--        <tr onmouseout="nightsight_stop()" onmouseover="nightsight_start()">-->
<!--            <td style="padding:20px;width:25%;vertical-align:middle">-->
<!--              <img src="images/meta-adr.png" alt="hpp" style="border-style: none" width="200">-->
<!--            </td>-->
<!--            <td style="padding:20px;width:75%;vertical-align:middle">-->
<!--              <a href="https://arxiv.org/abs/2112.07066">-->
<!--                  <papertitle>Curriculum in Gradient-Based Meta-Reinforcement Learning</papertitle>-->
<!--              </a>-->
<!--              <br>-->
<!--                Bhairav Mehta, Tristan Deleu*, <strong>Sharath Chandra*</strong> Christopher Pal, Liam Paull-->
<!--                <br>-->
<!--              <em>ICLR BeTR-RL workshop (2021)</em>-->
<!--                <br>-->
<!--              [-->
<!--              <a href="https://arxiv.org/abs/2002.07956">Paper</a>]-->
<!--                <br>-->
<!--                <p>-->
<!--                    In this work we study the under-studied parameter in meta learning, "Task Distributions". We show that Model Agnostic Meta-Learning (MAML) is sensitive to task distributions, and learning a curriculum of tasks instead of uniformly sampling helps the adaptation performance substantially.-->

<!--                </p>-->
<!--            </td>-->
<!--          </tr>-->
        <tr onmouseout="nightsight_stop()" onmouseover="nightsight_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/r:ss.png" alt="hpp" style="border-style: none" width="200">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/2112.07066">
                  <papertitle>CuNAS - CUriosity-driven Neural-Augmented Simulator</papertitle>
              </a>
              <br>
                <strong>Sharath Chandra Raparthy</strong>, Melissa Mozifian, Liam Paull and Florian Golemo
                <br>
              <em>RSS Sim2Real workshop (2021)</em>
                <br>
              [
              <a href="https://docs.google.com/presentation/d/1nVbt0iQKFTOgHEQLLHbn1Wy3bMs_mWpyzfc0aZsN30U/edit?usp=sharing">Slides</a> /
              <a href="https://www.youtube.com/watch?v=Tlf5RG3OPF8&list=PL4BpvvbNDc3SxmswMbOljlUcCQJQ6eFDL&index=6">Talk</a>]
              <br>
              <p>Transfer of policies from simulation to physical robots is an important open problem in deep reinforcement learning. Prior work has introduced the model-based Neural-Augmented Simulator (NAS) method, which uses task-independent data to create a model of the differences between simulated and real robot. In this work, we show that this method is sensitive to the sampling of motor actions and the control frequency. To overcome this problem, we propose a simple extension based on artificial curiosity. We demonstrate on a physical robot, that this leads to a better exploration of the state space and consequently better transfer performance when compared to the NAS baseline.</p>
            </td>
          </tr>

        </tbody></table>

      </td>
    </tr>
  </table>
  Template from <a href="https://jonbarron.info/">this website</a>.
</body>

</html>
